{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-39-239.us-west-2.compute.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=PySparkShell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row,SQLContext\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.feature import CountVectorizer, Tokenizer, StringIndexer, \\\n",
    "    NGram, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import countDistinct, udf, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loads parquet file located in AWS S3 into RDD Data Frame\n",
    "parquetFileTip = sqlContext.read.parquet(\"s3://dknsyelp/tip.parquet\")\n",
    "# Stores the DataFrame into an \"in-memory temporary table\"\n",
    "parquetFileTip.registerTempTable(\"parquetFileTip\")\n",
    "# Run standard SQL queries against temporary table\n",
    "df_tip = sqlContext.sql(\"SELECT * FROM parquetFileTip\")\n",
    "\n",
    "parquetFileReview = sqlContext.read.parquet(\"s3://dknsyelp/review.parquet\")\n",
    "parquetFileReview.registerTempTable(\"parquetFileReview\")\n",
    "df_review = sqlContext.sql(\"SELECT * FROM parquetFileReview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_review = df_review.select(df_review['text'], df_review['stars']).dropDuplicates()\n",
    "# Replace all punctuation\n",
    "import string\n",
    "import re\n",
    "to_remove = string.punctuation + '0-9\\\\r\\\\t\\\\n'\n",
    "to_remove = r\"[{}]\".format(to_remove)       # correct format for regex\n",
    "my_regex = re.compile(to_remove)\n",
    "# Replace instances of every element in to_remove with empty string\n",
    "text_clean = udf(lambda text: my_regex.sub('', text))\n",
    "df_review = df_review.withColumn('clean_text', text_clean(col('text'))).drop('text').withColumnRenamed(\"clean_text\", \"text\").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data for Training the Naive Bayes Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create Labels for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts of reviews by stars:\n",
      "+-----+--------------------+\n",
      "|stars|count(DISTINCT text)|\n",
      "+-----+--------------------+\n",
      "|    1|              122600|\n",
      "|    3|              104366|\n",
      "|    5|              357558|\n",
      "|    4|              204525|\n",
      "|    2|               73779|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print 'counts of reviews by stars:'\n",
    "df_review.groupBy(df_review.stars).agg(countDistinct(df_review.text)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Treat 1, 2, 3 star ratings as 'bad' reviews & 4, 5 star ratings as 'good' reviews\n",
    "binning_udf = udf(lambda x: 1 if x > 3 else 0)\n",
    "df_review = df_review.withColumn('label', binning_udf(col('stars')))\n",
    "# convert label to integer type\n",
    "df_review = df_review.withColumn(\"label\", df_review[\"label\"].cast(IntegerType())).drop('stars')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[text: string, label: int]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, valid_data = df_review.randomSplit([0.8, 0.2])\n",
    "train_data.cache()\n",
    "valid_data.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nb_train_pipeline(n=3):\n",
    "    # convert the input string to lowercase and split it by spaces\n",
    "    tokenizer = [Tokenizer(inputCol=\"text\", outputCol=\"words\")]\n",
    "    ngrams = [NGram(n=i, inputCol=\"words\", outputCol=\"{0}_grams\".format(i)) for i in range(1, n + 1)]\n",
    "    # create vocabulary (use binary=True for Binarized Multinomial Naive Bayes)\n",
    "    vectorizers = [CountVectorizer(inputCol=\"{0}_grams\".format(i), \n",
    "                                   outputCol=\"{0}_counts\".format(i), binary=True) for i in range(1, n + 1)]\n",
    "    # combine all n-grams\n",
    "    assembler = [VectorAssembler(inputCols=[\"{0}_counts\".format(i) for i in range(1, n + 1)], outputCol=\"features\")]\n",
    "    nb = [NaiveBayes(smoothing=1.0, modelType=\"multinomial\")]\n",
    "    return Pipeline(stages=tokenizer + ngrams + vectorizers + assembler + nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = nb_train_pipeline(n=3).fit(train_data)\n",
    "preds_valid = model.transform(valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area Under ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "areaUnderROC:0.613\n"
     ]
    }
   ],
   "source": [
    "bceval = BinaryClassificationEvaluator()\n",
    "print bceval.getMetricName() +\":\" + str(round(bceval.evaluate(preds_valid), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area Under PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "areaUnderPR:0.729\n"
     ]
    }
   ],
   "source": [
    "bceval.setMetricName(\"areaUnderPR\")\n",
    "print bceval.getMetricName() +\":\" + str(round(bceval.evaluate(preds_valid), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1:0.889\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model. metric : F1 score...... f1:0.865\n",
    "# with text_clean: 0.858\n",
    "# with text_clean + nb_train_pipeline(n=2): 0.882\n",
    "mceval = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "print mceval.getMetricName() +\":\" + str(round(mceval.evaluate(preds_valid), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.89\n"
     ]
    }
   ],
   "source": [
    "mceval.setMetricName(\"accuracy\")\n",
    "print (mceval.getMetricName() +\":\" + str(round(mceval.evaluate(preds_valid), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model using All Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = nb_train_pipeline(n=3).fit(df_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions on the Tips Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tips.json` file includes tips (short reviews) for restaurants.  However, ratings are not available for each tip/review. Here, we use the Naive Bayes model we trained on the data from `reviews.json` (containing full reviews + ratings for restaurants) and use the model to make predictions of the sentiment of the \"tips\" left by users.  Finally, we can convert the predicted probability of a tip being positive into star reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+-----+--------------------+--------+-------+\n",
      "|         business_id|             user_id|                 _id|      date|likes|                text| user_ix| biz_ix|\n",
      "+--------------------+--------------------+--------------------+----------+-----+--------------------+--------+-------+\n",
      "|-8fOqUWFX_1qiKggI...|UcAQjopfBULit3uVL...|[5a5eb6ea07002a9b...|2013-01-19|    0|\"It's our pleasur...|454892.0|28595.0|\n",
      "|-8fOqUWFX_1qiKggI...|mmnKrqV1W8WSVH5vK...|[5a5eb6ea07002a9b...|2015-08-03|    0|Quick service, ho...|430157.0|28595.0|\n",
      "|-8fOqUWFX_1qiKggI...|sjSXoxakmlFuVHQlp...|[5a5eb6e207002a9b...|2010-08-10|    0|Kids can play, cl...|190165.0|28595.0|\n",
      "|-8fOqUWFX_1qiKggI...|Cl2pyB8mWn3RCMV5n...|[5a5eb6e207002a9b...|2012-06-22|    0|Watch out for tra...|429184.0|28595.0|\n",
      "|-8fOqUWFX_1qiKggI...|Cl2pyB8mWn3RCMV5n...|[5a5eb6e207002a9b...|2012-07-16|    0|Milkshake Mondays...|429184.0|28595.0|\n",
      "+--------------------+--------------------+--------------------+----------+-----+--------------------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tip.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Tips Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tip = df_tip.select('user_ix', 'biz_ix', 'text').dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace instances of every element in to_remove with empty string\n",
    "df_tip = df_tip.withColumn('clean_text', text_clean(col('text'))).drop('text').withColumnRenamed(\"clean_text\", \"text\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------------------+\n",
      "| user_ix|  biz_ix|                text|\n",
      "+--------+--------+--------------------+\n",
      "|249438.0| 25645.0|Pork Adobo with s...|\n",
      "|144775.0| 23984.0|         Great gyros|\n",
      "|207763.0| 33687.0|Great doctor Goin...|\n",
      "|523391.0| 93635.0|Bulls choked  Ros...|\n",
      "|152442.0| 45971.0|AYCE is the only ...|\n",
      "| 62412.0| 79963.0|Hit the dessert b...|\n",
      "|195045.0|118722.0|Make sure the hea...|\n",
      "|132911.0|118722.0|              Smelly|\n",
      "|178406.0|118722.0|Nice rooms the ma...|\n",
      "| 75242.0|118722.0|Last stop on the ...|\n",
      "|125925.0| 15184.0|Cool atmosphere a...|\n",
      "|365503.0| 74326.0|Yipee Its my Aloh...|\n",
      "|439386.0| 74326.0|Late night shoppi...|\n",
      "|217030.0|  4773.0|Owl of Minerva in...|\n",
      "|253770.0| 84624.0|Most sickening re...|\n",
      "|149331.0| 84624.0|No Dont do it Go ...|\n",
      "|146661.0| 69781.0|Excellent burgers...|\n",
      "|259265.0| 17132.0|The red house win...|\n",
      "|220735.0|   607.0|Ask for Jamie whe...|\n",
      "| 77784.0|  1438.0|Its not closed to...|\n",
      "+--------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tip.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Ratings using Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_test = model.transform(df_tip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_ix: double (nullable = true)\n",
      " |-- biz_ix: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- 1_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 2_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 3_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 1_counts: vector (nullable = true)\n",
      " |-- 2_counts: vector (nullable = true)\n",
      " |-- 3_counts: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+----------+\n",
      "|probability                                |prediction|\n",
      "+-------------------------------------------+----------+\n",
      "|[0.5278635068641435,0.47213649313585637]   |0.0       |\n",
      "|[0.04387179250090863,0.9561282074990914]   |1.0       |\n",
      "|[0.08834707434778237,0.9116529256522177]   |1.0       |\n",
      "|[0.07224206044503252,0.9277579395549674]   |1.0       |\n",
      "|[0.23726898700743274,0.7627310129925673]   |1.0       |\n",
      "|[0.09332752385244389,0.9066724761475562]   |1.0       |\n",
      "|[0.013316465612433367,0.9866835343875665]  |1.0       |\n",
      "|[0.6139301677384645,0.3860698322615355]    |0.0       |\n",
      "|[0.025529287134888737,0.9744707128651112]  |1.0       |\n",
      "|[0.0024199363550010816,0.997580063644999]  |1.0       |\n",
      "|[5.119764214083161E-23,1.0]                |1.0       |\n",
      "|[0.005206494423857925,0.994793505576142]   |1.0       |\n",
      "|[0.24394183604055572,0.7560581639594443]   |1.0       |\n",
      "|[2.2412511803097664E-7,0.999999775874882]  |1.0       |\n",
      "|[0.6474195341771555,0.3525804658228446]    |0.0       |\n",
      "|[0.9962325032913845,0.003767496708615598]  |0.0       |\n",
      "|[1.529163212405181E-5,0.9999847083678759]  |1.0       |\n",
      "|[2.325822564003444E-4,0.9997674177435998]  |1.0       |\n",
      "|[1.4421224459306076E-15,0.9999999999999987]|1.0       |\n",
      "|[0.7692647686848768,0.23073523131512322]   |0.0       |\n",
      "+-------------------------------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds_test.select('probability', 'prediction').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(probability=DenseVector([0.5279, 0.4721])),\n",
       " Row(probability=DenseVector([0.0439, 0.9561])),\n",
       " Row(probability=DenseVector([0.0883, 0.9117])),\n",
       " Row(probability=DenseVector([0.0722, 0.9278])),\n",
       " Row(probability=DenseVector([0.2373, 0.7627]))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_test.select('probability', 'prediction').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(preds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_test.write.parquet(\"s3://dknsyelp/preds_test.parquet\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_test = sqlContext.read.parquet(\"s3://dknsyelp/preds_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ratings(problist):\n",
    "    prob_positive = problist[1]\n",
    "    rating = 1\n",
    "    if 0.2 < prob_positive <= 0.4:\n",
    "        rating = 2\n",
    "    elif 0.4 < prob_positive <= 0.6:\n",
    "        rating = 3\n",
    "    elif 0.6 < prob_positive <= 0.8:\n",
    "        rating = 4\n",
    "    elif 0.8 < prob_positive:\n",
    "        rating = 5\n",
    "    return rating\n",
    "\n",
    "get_ratings_udf = udf(lambda x: get_ratings(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_df = preds_test.select('biz_ix', 'user_ix', 'probability')\n",
    "pred_df = pred_df.withColumn('stars', get_ratings_udf(col('probability')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-------------------------------------------+-----+\n",
      "|biz_ix  |user_ix |probability                                |stars|\n",
      "+--------+--------+-------------------------------------------+-----+\n",
      "|32321.0 |201498.0|[2.644997420539402E-4,0.999735500257946]   |5    |\n",
      "|114236.0|12019.0 |[0.13097987356047133,0.8690201264395286]   |5    |\n",
      "|2671.0  |439127.0|[0.26264037708196536,0.7373596229180346]   |4    |\n",
      "|14767.0 |179368.0|[0.007764449068323095,0.9922355509316769]  |5    |\n",
      "|83636.0 |291490.0|[1.1974511383295448E-10,0.9999999998802549]|5    |\n",
      "|79593.0 |281722.0|[6.187727967169642E-9,0.9999999938122719]  |5    |\n",
      "|76403.0 |532075.0|[3.433558889126534E-6,0.9999965664411109]  |5    |\n",
      "|13165.0 |535189.0|[1.3304242765850174E-5,0.9999866957572342] |5    |\n",
      "|51748.0 |250862.0|[7.062982599137161E-11,0.9999999999293703] |5    |\n",
      "|54948.0 |75633.0 |[2.1453685483600898E-5,0.9999785463145163] |5    |\n",
      "|61308.0 |350016.0|[0.013486783177591982,0.986513216822408]   |5    |\n",
      "|98539.0 |281879.0|[0.05985758395544397,0.9401424160445561]   |5    |\n",
      "|72288.0 |116979.0|[0.9995733368276165,4.266631723834983E-4]  |1    |\n",
      "|33841.0 |280671.0|[7.1026459221453965E-6,0.9999928973540778] |5    |\n",
      "|68200.0 |7485.0  |[0.42325437865558363,0.5767456213444164]   |3    |\n",
      "|41247.0 |6454.0  |[3.279663129780154E-4,0.999672033687022]   |5    |\n",
      "|108916.0|375785.0|[0.06061345975363519,0.9393865402463649]   |5    |\n",
      "|36377.0 |374286.0|[0.9690676395125848,0.03093236048741514]   |1    |\n",
      "|114304.0|485051.0|[0.029386249819386753,0.9706137501806132]  |5    |\n",
      "|116507.0|314358.0|[0.04539682450035339,0.9546031754996467]   |5    |\n",
      "+--------+--------+-------------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tips_stars = pred_df.drop('probability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tips_stars.write.parquet(\"s3://dknsyelp/tips_stars.parquet\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
